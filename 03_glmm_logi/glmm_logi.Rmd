---
title: "Logistic Regression with Random Intercepts"
author: "Will Townes"
date: "May 7, 2016"
output: html_document
---

```{r}
library(rstan)
library(reshape2)
library(plyr)
library(lme4)
rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores())
set.seed(39)
```

### Model Specification

The outcome for $i^{th}$ observation in $k^{th}$ cluster is binary $Y_{ki}$ with $P(Y_{ki}=1)=g(X_{ki}\beta+\gamma_k)$ where $\beta$ are the fixed effects, $X_{ki}$ is the vector of covariates, and $\gamma_k$ is a random intercept shared among all observations in the cluster. In traditional mixed models, we assume $\gamma_k\sim\mathcal{N}(0,\sigma^2_\gamma)$ but in the Bayesian context we can specify more flexible distributions for $\gamma_k$.

### Generating Simulated Data

```{r}
expit<-function(x){1/(1+exp(-x))}
K<-100 #clusters
n<-10 #observations/cluster
beta_true=c(-2,1,.5) #fixed effects
sigma<-2 #random effects standard deviation
dat<-data.frame(id=factor(rep(1:K,each=n)))
x1<-rbinom(K,1,.5)
dat$x1<-rep(x1,each=n) #cluster-specific covar
dat$x2<-rep(seq(from=-4.5,to=4.5),K) #within-cluster covar
dat<-within(dat,eta<-beta_true[1]+x1*beta_true[2]+x2*beta_true[3])
#random intercepts
bdat<-data.frame(id=factor(1:K),b=rnorm(K,0,sigma)) 
dat<-join(dat,bdat,by="id")
dat<-within(dat,mu<-expit(eta+b))
dat$y<-rbinom(K*n,1,dat$mu)
dat_obs<-dat[,c("id","x1","x2","y")]
```
First we will use the standard frequentist approach (maximum likelihood) to get a baseline for comparison.
```{r}
frq<-glmer(y~x1+x2+(1|id),data=dat_obs,family=binomial)
summary(frq)
```
Recall the true fixed effects are $-2,1,.5$ and the random effect standard deviation is $2$.

### Stan Program

Run the stan program for random intercepts logistic regression. There is no need for data in a list like in the example on their website, since stan searches the calling environment. The stan code is in a separate file. Commonly, one may specify a normal distribution with mean zero and variance $\sigma^2$ as the random effects distribution, and then place an inverse gamma conjugate prior on $\sigma^2$ with shape $a$ and scale $b$. However, this scheme can be analytically marginalized out if $\sigma^2$ is not of intrinsic interest. If $\sigma^2\sim\mathcal{IG}(a,b)$ then set $\nu=2a$ and $\tau^2=a/b$ so that $\sigma^2\sim\mathcal{IG}(\nu/2,\nu \tau^2/2)$. This is equivalent to a scaled inverse chi-squared prior, with the property that $\tau^2=E[\sigma^2]$. Analytically integrating out $\sigma^2$, we obtain a marginal Student's t prior for the random effects with degrees of freedom $\nu$, mean zero, and scale parameter $\tau^2$.

```{r}
#row_vector[D] X[N]; #covariate design matrix
X<-as.matrix(dat_obs[,c("x1","x2")])
D<-ncol(X)
N<-nrow(X)
y<-dat_obs$y #force numeric
id<-as.numeric(dat_obs$id)
system.time(stfit<-stan(file="glmm_logi.stan",iter=100,chains=1))
```
I noticed that the time was about 20 seconds the first time I ran the command, probably due to the C++ program compiling. But when I ran the command again, it only took 5 seconds. This is still a lot slower than the frequentist approach. Next, let's explore the output of the Stan program.
```{r}
print(lrfit) #lrfit is S4 object of class stanfit
#help("stanfit-class")
#get posterior means
#summary(lrfit)
stan_params<-summary(lrfit)$summary
stan_signif<-stan_params[,"2.5%"]*stan_params[,"97.5%"]>0
```
Stan found variables 4,5,6 to be not significant from zero versus true value being 5,6. The frequentist approach (OLS) made the same mistake (see above). Let's look at some pretty graphs!
```{r,echo=FALSE}
quietgg(plot(lrfit,plotfun="hist",pars=NULL,include=FALSE))
pairs(lrfit,pars=c("alpha","beta[1]","beta[7]","sigma")) #rather slow.
```
We now compare the relative errors of stan and lm
```{r}
stan_params<-stan_params[-12,1]
stan_err<-stan_params-true_params
stan_rel_err<-stan_err/true_params

rel_err<-data.frame(var=names(stan_params),OLS=frq_rel_err,STAN=stan_rel_err)
plot_dat<-melt(rel_err,variable.name="method",id.vars="var",value.name="rel_err")
ggplot(data=plot_dat,aes(var,rel_err))+geom_bar(aes(fill=method),position="dodge",stat="identity")+theme_bw()
ggplot(data=plot_dat[-c(6,7,17,18),],aes(var,rel_err))+geom_bar(aes(fill=method),position="dodge",stat="identity")+theme_bw()

mse_frq<-sum(frq_err^2)/11
mse_stan<-sum(stan_err^2)/11
mse_frq
mse_stan
```
We see that the stan method gives very similar results to the OLS method due to the uninformative prior.

### Coming Soon- more interesting models!
I will use this template to practice with Stan in more interesting and complicated/ useful models in the future.